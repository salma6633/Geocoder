{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "# Define the path to the ZIP file in your Drive\n",
        "zip_path = \"/content/drive/MyDrive/models.zip\"  # Change if it's in a subfolder\n",
        "\n",
        "# Create a target directory\n",
        "!mkdir -p models\n",
        "\n",
        "# Unzip the file\n",
        "!unzip -q \"{zip_path}\" -d models\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4eHvhe2ifa2",
        "outputId": "9df6c048-a4a2-4523-bb97-ea827201471e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UQsw_UITy6WR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary dependencies\n",
        "!pip install -q numpy pandas joblib faiss-cpu sentence-transformers scikit-learn fuzzywuzzy xgboost python-Levenshtein\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBQzCMVNskA3",
        "outputId": "03b907c7-9481-416d-8287-17ab40814823"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m159.9/159.9 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fuzzy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCWvWMCZyUMM",
        "outputId": "450e65a0-001e-4a43-de2d-a339509b014d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fuzzy\n",
            "  Downloading Fuzzy-1.2.2.tar.gz (14 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: fuzzy\n",
            "  Building wheel for fuzzy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fuzzy: filename=Fuzzy-1.2.2-cp312-cp312-linux_x86_64.whl size=231377 sha256=394be6a42a7d984d202d6baf056d101b9155eb216135f80848497ac7e44ae31e\n",
            "  Stored in directory: /root/.cache/pip/wheels/ae/17/05/e688b455079b61cb255bc2e08458d6e66894a14be5cd5a5954\n",
            "Successfully built fuzzy\n",
            "Installing collected packages: fuzzy\n",
            "Successfully installed fuzzy-1.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import os\n",
        "import json\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import faiss\n",
        "import re\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from fuzzywuzzy import process\n",
        "import fuzzy\n",
        "import warnings\n",
        "from math import radians, cos, sin, asin, sqrt\n",
        "from functools import lru_cache\n",
        "import xgboost as xgb\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "JC_Mz-axDrG-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Global SentenceTransformer to avoid reloading\n",
        "SENTENCE_EMBEDDER = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "\n",
        "# Define EnhancedXGBoostGeocoder class\n",
        "class EnhancedXGBoostGeocoder:\n",
        "    def __init__(self):\n",
        "        self.lat_model = None\n",
        "        self.lon_model = None\n",
        "\n",
        "    def train(self, X_train, y_train, X_val=None, y_val=None, weights_train=None):\n",
        "        print(\"Training Enhanced XGBoost Geocoder...\")\n",
        "        params = {\n",
        "            'objective': 'reg:squarederror',\n",
        "            'n_estimators': 800,\n",
        "            'max_depth': 6,\n",
        "            'learning_rate': 0.1,\n",
        "            'subsample': 0.8,\n",
        "            'colsample_bytree': 0.8,\n",
        "            'tree_method': 'hist',\n",
        "            'random_state': 42\n",
        "        }\n",
        "        self.lat_model = xgb.XGBRegressor(**params)\n",
        "        self.lon_model = xgb.XGBRegressor(**params)\n",
        "        self.lat_model.fit(X_train, y_train[:, 0], sample_weight=weights_train)\n",
        "        self.lon_model.fit(X_train, y_train[:, 1], sample_weight=weights_train)\n",
        "        if X_val is not None and y_val is not None:\n",
        "            print(\"Evaluating on validation set...\")\n",
        "            val_pred = self.predict(X_val)\n",
        "            from sklearn.metrics import mean_squared_error\n",
        "            mse_lat = mean_squared_error(y_val[:, 0], val_pred[:, 0])\n",
        "            mse_lon = mean_squared_error(y_val[:, 1], val_pred[:, 1])\n",
        "            print(f\"Validation MSE - Latitude: {mse_lat:.6f}, Longitude: {mse_lon:.6f}\")\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        lat_pred = self.lat_model.predict(X)\n",
        "        lon_pred = self.lon_model.predict(X)\n",
        "        return np.column_stack((lat_pred, lon_pred))\n",
        "\n",
        "# Define HybridXGBFAISSGeocoder class\n",
        "class HybridXGBFAISSGeocoder:\n",
        "    def __init__(self, k_neighbors=3, models_dir=\"models\"):\n",
        "        self.xgb_geocoder = EnhancedXGBoostGeocoder()\n",
        "        self.faiss_lat_index = None\n",
        "        self.faiss_lon_index = None\n",
        "        self.residuals = None\n",
        "        self.k = k_neighbors\n",
        "        self.X_train_ref = None\n",
        "        self.weights_train = None\n",
        "        self.distance_weighting = 'inverse_squared'\n",
        "        self.residual_cap = 0.01\n",
        "        self.models_dir = models_dir\n",
        "\n",
        "    def train(self, X_train, y_train, X_val=None, y_val=None, weights_train=None):\n",
        "        print(\"ðŸ”¥ Training Hybrid XGBoost + FAISS model...\")\n",
        "        self.xgb_geocoder.train(X_train, y_train, X_val, y_val, weights_train=weights_train)\n",
        "        xgb_pred = self.xgb_geocoder.predict(X_train)\n",
        "        self.residuals = y_train - xgb_pred\n",
        "        self.weights_train = weights_train\n",
        "        d = X_train.shape[1]\n",
        "        X_train_f32 = X_train.astype(np.float32)\n",
        "        self.faiss_lat_index = faiss.IndexFlatL2(d)\n",
        "        self.faiss_lon_index = faiss.IndexFlatL2(d)\n",
        "        self.faiss_lat_index.add(X_train_f32)\n",
        "        self.faiss_lon_index.add(X_train_f32)\n",
        "        self.X_train_ref = X_train_f32\n",
        "        os.makedirs(self.models_dir, exist_ok=True)\n",
        "        faiss.write_index(self.faiss_lat_index, os.path.join(self.models_dir, 'faiss_lat_index.index'))\n",
        "        faiss.write_index(self.faiss_lon_index, os.path.join(self.models_dir, 'faiss_lon_index.index'))\n",
        "        np.save(os.path.join(self.models_dir, 'X_train_ref.npy'), X_train_f32)\n",
        "        np.save(os.path.join(self.models_dir, 'residuals.npy'), self.residuals)\n",
        "        if self.weights_train is not None:\n",
        "            np.save(os.path.join(self.models_dir, 'sample_weights.npy'), self.weights_train)\n",
        "        self.faiss_lat_index = None\n",
        "        self.faiss_lon_index = None\n",
        "        self.X_train_ref = None\n",
        "        self.residuals = None\n",
        "        self.weights_train = None\n",
        "        print(f\"âœ… FAISS indexes built with {len(X_train)} reference points\")\n",
        "        return self\n",
        "\n",
        "    def load_artifacts(self):\n",
        "        self.faiss_lat_index = faiss.read_index(os.path.join(self.models_dir, 'faiss_lat_index.index'))\n",
        "        self.faiss_lon_index = faiss.read_index(os.path.join(self.models_dir, 'faiss_lon_index.index'))\n",
        "        self.X_train_ref = np.load(os.path.join(self.models_dir, 'X_train_ref.npy'))\n",
        "        self.residuals = np.load(os.path.join(self.models_dir, 'residuals.npy'))\n",
        "        weights_path = os.path.join(self.models_dir, 'sample_weights.npy')\n",
        "        if os.path.exists(weights_path):\n",
        "            self.weights_train = np.load(weights_path)\n",
        "        else:\n",
        "            self.weights_train = None\n",
        "\n",
        "    def predict(self, X):\n",
        "        if self.xgb_geocoder.lat_model is None or self.xgb_geocoder.lon_model is None:\n",
        "            raise ValueError(\"Models not trained yet. Call train() first.\")\n",
        "        if self.faiss_lat_index is None:\n",
        "            self.load_artifacts()\n",
        "        xgb_pred = self.xgb_geocoder.predict(X)\n",
        "        X_f32 = X.astype(np.float32)\n",
        "        dist_lat, idx_lat = self.faiss_lat_index.search(X_f32, self.k)\n",
        "        dist_lon, idx_lon = self.faiss_lon_index.search(X_f32, self.k)\n",
        "        eps = 1e-8\n",
        "        weights_lat = 1 / (dist_lat + eps)\n",
        "        weights_lon = 1 / (dist_lon + eps)\n",
        "        weights_lat /= weights_lat.sum(axis=1, keepdims=True)\n",
        "        weights_lon /= weights_lon.sum(axis=1, keepdims=True)\n",
        "        if self.weights_train is not None:\n",
        "            weights_lat = weights_lat * self.weights_train[idx_lat]\n",
        "            weights_lon = weights_lon * self.weights_train[idx_lon]\n",
        "            weights_lat /= weights_lat.sum(axis=1, keepdims=True)\n",
        "            weights_lon /= weights_lon.sum(axis=1, keepdims=True)\n",
        "        correction_lat = (weights_lat * self.residuals[idx_lat, 0]).sum(axis=1)\n",
        "        correction_lon = (weights_lon * self.residuals[idx_lon, 1]).sum(axis=1)\n",
        "        final_pred = xgb_pred + np.column_stack((correction_lat, correction_lon))\n",
        "        self.faiss_lat_index = None\n",
        "        self.faiss_lon_index = None\n",
        "        self.X_train_ref = None\n",
        "        self.residuals = None\n",
        "        self.weights_train = None\n",
        "        return final_pred\n",
        "\n",
        "# Define FixedHybridGeocoder class\n",
        "class FixedHybridGeocoder:\n",
        "    def __init__(self, models_dir: str = \"models\"):\n",
        "        self.models_dir = models_dir\n",
        "        self.artifacts = {}\n",
        "        self.is_loaded = False\n",
        "        self.kuwait_governorates = {}\n",
        "        self.abbreviation_map = {}\n",
        "        self.common_typos = {}\n",
        "        self.all_kuwait_areas = []\n",
        "        self.typo_patterns = []\n",
        "        self.kuwait_bounds = {\n",
        "            'lat_min': 28.524574,\n",
        "            'lat_max': 30.103532,\n",
        "            'lon_min': 46.552695,\n",
        "            'lon_max': 48.416094\n",
        "        }\n",
        "        self.kuwait_center = {\n",
        "            'latitude': 29.3759,\n",
        "            'longitude': 47.9774\n",
        "        }\n",
        "        self.area_similarity_cache = {}\n",
        "\n",
        "    def load_artifacts(self):\n",
        "        try:\n",
        "            self.artifacts['feature_scaler'] = joblib.load(os.path.join(self.models_dir, 'feature_scaler.pkl'))\n",
        "            self.artifacts['tfidf_vectorizer'] = joblib.load(os.path.join(self.models_dir, 'tfidf_vectorizer.pkl'))\n",
        "            self.artifacts['manual_feature_columns'] = joblib.load(os.path.join(self.models_dir, 'manual_feature_columns.pkl'))\n",
        "            self.artifacts['sentence_embedder'] = SENTENCE_EMBEDDER\n",
        "            self.artifacts['geo_stats'] = joblib.load(os.path.join(self.models_dir, 'geo_stats.pkl'))\n",
        "            with open(os.path.join(self.models_dir, 'address_normalization_dicts.json'), 'r', encoding='utf-8') as f:\n",
        "                address_dicts = json.load(f)\n",
        "            self.kuwait_governorates = address_dicts['kuwait_governorates']\n",
        "            self.abbreviation_map = address_dicts['abbreviation_map']\n",
        "            self.common_typos = address_dicts['common_typos']\n",
        "            if 'sharq' in self.abbreviation_map:\n",
        "                del self.abbreviation_map['sharq']\n",
        "            if 'sharq' in self.common_typos:\n",
        "                self.common_typos['sharq'] = 'sharq'\n",
        "            self.common_typos = {k: v for k, v in self.common_typos.items() if len(k) > 1}\n",
        "            self.all_kuwait_areas = [area for gov_areas in self.kuwait_governorates.values() for area in gov_areas]\n",
        "            self.all_kuwait_areas.extend([correct for typo, correct in self.common_typos.items() if correct not in self.all_kuwait_areas])\n",
        "            self.all_kuwait_areas = list(set(self.normalize_text(area) for area in self.all_kuwait_areas))\n",
        "            self.typo_patterns = [(re.compile(rf\"\\\\b{re.escape(typo)}\\\\b\", re.IGNORECASE), correct) for typo, correct in sorted(self.common_typos.items(), key=lambda x: len(x[0]), reverse=True)]\n",
        "            with open(os.path.join(self.models_dir, 'training_metadata.json'), 'r') as f:\n",
        "                self.artifacts['metadata'] = json.load(f)\n",
        "            self.artifacts['hybrid_model'] = joblib.load(os.path.join(self.models_dir, 'hybrid_xgbfaiss_geocoder.pkl'))\n",
        "            self._validate_loaded_components()\n",
        "            self.is_loaded = True\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Failed to load artifacts: {e}\")\n",
        "\n",
        "    def _validate_loaded_components(self):\n",
        "        expected_dims = self.artifacts['metadata']['feature_dimensions']\n",
        "        actual_dims = self.artifacts['hybrid_model'].xgb_geocoder.lat_model.n_features_in_\n",
        "        if expected_dims != actual_dims:\n",
        "            raise ValueError(f\"Feature dimension mismatch: expected {expected_dims}, got {actual_dims}\")\n",
        "\n",
        "    def normalize_text(self, text: str) -> str:\n",
        "        if not text or pd.isna(text):\n",
        "            return \"\"\n",
        "        text = str(text).strip().translate(str.maketrans(\"Ù Ù¡Ù¢Ù£Ù¤Ù¥Ù¦Ù§Ù¨Ù©\", \"0123456789\")).translate(str.maketrans(\"Ø¥Ø£Ø¢Ø§Ù‰Ø¦Ø¡Ø¤\", \"Ø§Ø§Ø§Ø§ÙŠØ¡Ø¡Ø¡\"))\n",
        "        for pattern, correct in self.typo_patterns:\n",
        "            text = pattern.sub(correct, text)\n",
        "        text = re.sub(r\"[^\\w\\s\\d\\u0600-\\u06FF]\", \" \", text.lower())\n",
        "        words = [self.abbreviation_map.get(word, word) for word in text.split()]\n",
        "        return \" \".join(words)\n",
        "\n",
        "    def validate_kuwaiti_block(self, block: str) -> bool:\n",
        "        if not block or pd.isna(block):\n",
        "            return False\n",
        "        return bool(re.fullmatch(r\"^\\d{1,3}[a-zA-Z]?$\", str(block).strip()))\n",
        "\n",
        "    def validate_kuwaiti_street(self, street: str) -> bool:\n",
        "        if not street or pd.isna(street):\n",
        "            return False\n",
        "        street = self.normalize_text(street)\n",
        "        return bool(re.fullmatch(\n",
        "            r\"^(?:street|avenue|road|lane|crescent|Ø´Ø§Ø±Ø¹|Ø¬Ø§Ø¯Ø©|Ø·Ø±ÙŠÙ‚|Ø­Ø§Ø±Ø©|Ù‡Ù„Ø§Ù„)?\\s*[\\d\\w\\s\\-]+$\",\n",
        "            street, re.IGNORECASE\n",
        "        )) and len(street) <= 100\n",
        "\n",
        "    def categorize_street(self, street: str) -> str:\n",
        "        street = self.normalize_text(street)\n",
        "        if street == \"unknown\" or not street:\n",
        "            return \"unknown\"\n",
        "        if re.match(r\"^(?:street|avenue|road|lane|crescent|Ø´Ø§Ø±Ø¹|Ø¬Ø§Ø¯Ø©|Ø·Ø±ÙŠÙ‚|Ø­Ø§Ø±Ø©|Ù‡Ù„Ø§Ù„)?\\s*\\d+$\", street, re.IGNORECASE):\n",
        "            return \"numbered\"\n",
        "        if re.match(r\"^(?:street|avenue|road|lane|crescent|Ø´Ø§Ø±Ø¹|Ø¬Ø§Ø¯Ø©|Ø·Ø±ÙŠÙ‚|Ø­Ø§Ø±Ø©|Ù‡Ù„Ø§Ù„)?\\s*[\\w\\s\\-]+$\", street, re.IGNORECASE):\n",
        "            return \"named\"\n",
        "        return \"unknown\"\n",
        "\n",
        "    def parse_address_robust(self, address: str) -> Dict:\n",
        "        if not address or not isinstance(address, str):\n",
        "            return {\n",
        "                'country': 'kuwait', 'area': 'unknown', 'block': 'unknown', 'street': 'unknown',\n",
        "                'buildingNumber': '', 'apartment': '', 'floor': ''\n",
        "            }\n",
        "        normalized_address = self.normalize_text(address)\n",
        "        result = {\n",
        "            'country': 'kuwait', 'area': 'unknown', 'block': 'unknown', 'street': 'unknown',\n",
        "            'buildingNumber': '', 'apartment': '', 'floor': ''\n",
        "        }\n",
        "        area_found = self.extract_area_advanced(normalized_address)\n",
        "        if area_found != 'unknown':\n",
        "            result['area'] = area_found\n",
        "        block_found = self.extract_block_robust(normalized_address)\n",
        "        if block_found != 'unknown':\n",
        "            result['block'] = block_found\n",
        "        street_found = self.extract_street_robust(normalized_address)\n",
        "        if street_found != 'unknown':\n",
        "            result['street'] = street_found\n",
        "        result.update(self.extract_other_components(normalized_address))\n",
        "        return result\n",
        "\n",
        "    def extract_area_advanced(self, text: str) -> str:\n",
        "        text = self.normalize_text(text)\n",
        "        text_words = set(text.split())\n",
        "        candidates = []\n",
        "        phonetic_areas = {fuzzy.nysiis(area): area for area in self.all_kuwait_areas}\n",
        "        text_phonetic = fuzzy.nysiis(text)\n",
        "        for area in self.all_kuwait_areas:\n",
        "            area_norm = self.normalize_text(area)\n",
        "            area_words = set(area_norm.split())\n",
        "            if not area_words:\n",
        "                continue\n",
        "            score = len(text_words & area_words) / len(area_words)\n",
        "            position = text.find(area_norm)\n",
        "            if score >= 0.5 and position != -1:\n",
        "                penalty = 0.1 if len(area_words) > 1 else 0.0\n",
        "                candidates.append((area_norm, score - penalty, position, len(area_words)))\n",
        "        if text_phonetic in phonetic_areas:\n",
        "            phonetic_match = phonetic_areas[text_phonetic]\n",
        "            candidates.append((phonetic_match, 0.9, 0, 1))\n",
        "        best_match = 'unknown'\n",
        "        if candidates:\n",
        "            candidates.sort(key=lambda x: (-x[1], x[2], x[3]))\n",
        "            best_match = candidates[0][0]\n",
        "        if best_match == 'unknown':\n",
        "            kuwait_city_norm = self.normalize_text('kuwait city')\n",
        "            if kuwait_city_norm in text and 'sharq' not in text and 'mubarak' not in text.lower():\n",
        "                best_match = kuwait_city_norm\n",
        "        if best_match == 'unknown':\n",
        "            matches = process.extract(text, self.all_kuwait_areas, limit=1)\n",
        "            if matches and matches[0][1] >= 70:\n",
        "                best_match = self.normalize_text(matches[0][0])\n",
        "        return best_match\n",
        "\n",
        "    def extract_block_robust(self, text: str) -> str:\n",
        "        text = self.normalize_text(text)\n",
        "        patterns = [\n",
        "            r'block\\s+(\\d{1,3}[a-zA-Z]?)',\n",
        "            r'blk\\s+(\\d{1,3}[a-zA-Z]?)',\n",
        "            r'b\\s+(\\d{1,3}[a-zA-Z]?)',\n",
        "            r'(\\d{1,3}[a-zA-Z]?)\\s*(?:street|st|avenue|ave|road|rd|lane|ln|crescent|cr|Ø´Ø§Ø±Ø¹|Ø¬Ø§Ø¯Ø©|Ø·Ø±ÙŠÙ‚|Ø­Ø§Ø±Ø©|Ù‡Ù„Ø§Ù„)'\n",
        "        ]\n",
        "        for pattern in patterns:\n",
        "            match = re.search(pattern, text, re.IGNORECASE)\n",
        "            if match and self.validate_kuwaiti_block(match.group(1)):\n",
        "                return match.group(1)\n",
        "        return 'unknown'\n",
        "\n",
        "    def extract_street_robust(self, text: str) -> str:\n",
        "        text = self.normalize_text(text)\n",
        "        patterns = [\n",
        "            r'(?:street|avenue|road|lane|crescent|Ø´Ø§Ø±Ø¹|Ø¬Ø§Ø¯Ø©|Ø·Ø±ÙŠÙ‚|Ø­Ø§Ø±Ø©|Ù‡Ù„Ø§Ù„)\\s+([\\w\\s\\-]+?)(?=\\s*(?:block|building\\s+\\d+|floor|apartment|apt|\\d+\\s*$|$))',\n",
        "            r'(?:st|ave|rd|ln|cr)\\s+([\\w\\s\\-]+?)(?=\\s*(?:block|building\\s+\\d+|floor|apartment|apt|\\d+\\s*$|$))',\n",
        "            r'([\\w\\s\\-]+?)\\s+(?:street|avenue|road|lane|crescent|st|ave|rd|ln|cr|Ø´Ø§Ø±Ø¹|Ø¬Ø§Ø¯Ø©|Ø·Ø±ÙŠÙ‚|Ø­Ø§Ø±Ø©|Ù‡Ù„Ø§Ù„)(?=\\s*(?:block|building\\s+\\d+|floor|apartment|apt|\\d+\\s*$|$))'\n",
        "        ]\n",
        "        for pattern in patterns:\n",
        "            match = re.search(pattern, text, re.IGNORECASE)\n",
        "            if match:\n",
        "                street = match.group(1).strip()\n",
        "                if (self.validate_kuwaiti_street(street) and\n",
        "                    not any(area in self.normalize_text(street) for area in self.all_kuwait_areas) and\n",
        "                    not any(kw in street.lower() for kw in ['block', 'building', 'floor', 'apartment', 'apt'])):\n",
        "                    return street\n",
        "        words = text.split()\n",
        "        street_keywords = ['street', 'avenue', 'road', 'lane', 'crescent', 'Ø´Ø§Ø±Ø¹', 'Ø¬Ø§Ø¯Ø©', 'Ø·Ø±ÙŠÙ‚', 'Ø­Ø§Ø±Ø©', 'Ù‡Ù„Ø§Ù„', 'st', 'ave', 'rd', 'ln', 'cr']\n",
        "        for i, word in enumerate(words):\n",
        "            if word.lower() in street_keywords:\n",
        "                street_words_after = []\n",
        "                for j in range(i + 1, len(words)):\n",
        "                    next_word = words[j].lower()\n",
        "                    if (next_word in ['block', 'building', 'floor', 'apartment', 'apt'] or\n",
        "                        re.match(r'^\\d+$', next_word) or\n",
        "                        self.normalize_text(next_word) in self.all_kuwait_areas):\n",
        "                        break\n",
        "                    street_words_after.append(words[j])\n",
        "                street_words_before = []\n",
        "                for j in range(i - 1, -1, -1):\n",
        "                    prev_word = words[j].lower()\n",
        "                    if j - 1 >= 0 and words[j - 1].lower() == 'block' and bool(re.fullmatch(r\"^\\d{1,3}[a-zA-Z]?$\", prev_word)):\n",
        "                        break\n",
        "                    if (prev_word in ['block', 'building', 'floor', 'apartment', 'apt'] or\n",
        "                        self.normalize_text(prev_word) in self.all_kuwait_areas):\n",
        "                        break\n",
        "                    street_words_before.insert(0, words[j])\n",
        "                street = ' '.join(street_words_before + street_words_after).strip()\n",
        "                if (street and self.validate_kuwaiti_street(street) and\n",
        "                    not any(area in self.normalize_text(street) for area in self.all_kuwait_areas) and\n",
        "                    not any(kw in street.lower() for kw in ['block', 'building', 'floor', 'apartment', 'apt'])):\n",
        "                    return street\n",
        "        return 'unknown'\n",
        "\n",
        "    def extract_other_components(self, text: str) -> Dict:\n",
        "        components = {}\n",
        "        text = self.normalize_text(text)\n",
        "        building_match = re.search(r'building\\s+(\\d+)|(\\d+)\\s*$', text, re.IGNORECASE)\n",
        "        if building_match:\n",
        "            components['buildingNumber'] = building_match.group(1) or building_match.group(2)\n",
        "        floor_match = re.search(r'floor\\s+(\\d+)', text, re.IGNORECASE)\n",
        "        if floor_match:\n",
        "            components['floor'] = floor_match.group(1)\n",
        "        apt_match = re.search(r'(?:apt|apartment)\\s+(\\w+)', text, re.IGNORECASE)\n",
        "        if apt_match:\n",
        "            components['apartment'] = apt_match.group(1)\n",
        "        return components\n",
        "\n",
        "    def get_governorate(self, area: str) -> str:\n",
        "        area_norm = self.normalize_text(area)\n",
        "        for gov, areas in self.kuwait_governorates.items():\n",
        "            if any(area_norm == self.normalize_text(a) for a in areas):\n",
        "                return gov\n",
        "        return \"unknown\"\n",
        "\n",
        "    def create_features_with_proper_fallbacks(self, addresses: List[str]) -> Tuple[np.ndarray, pd.DataFrame]:\n",
        "        parsed_data = []\n",
        "        for address in addresses:\n",
        "            parsed = self.parse_address_robust(address)\n",
        "            normalized_address = self.normalize_text(address)\n",
        "            normalized_address = re.sub(r'\\bbuilding\\s+\\d+\\b', '', normalized_address, flags=re.IGNORECASE).strip()\n",
        "            parsed['input_text'] = normalized_address\n",
        "            parsed['governorate'] = self.get_governorate(parsed['area'])\n",
        "            parsed['city'] = parsed['area']\n",
        "            parsed['area_normalized'] = self.normalize_text(parsed['area'])\n",
        "            area_key = (parsed['area_normalized'], tuple(self.all_kuwait_areas))\n",
        "            if area_key in self.area_similarity_cache:\n",
        "                parsed['area_similarity'] = self.area_similarity_cache[area_key]\n",
        "            else:\n",
        "                parsed['area_similarity'] = process.extractOne(parsed['area_normalized'], self.all_kuwait_areas)[1] / 100.0 if parsed['area'] != 'unknown' else 0.0\n",
        "                self.area_similarity_cache[area_key] = parsed['area_similarity']\n",
        "            parsed_data.append(parsed)\n",
        "        df = pd.DataFrame(parsed_data)\n",
        "        df['block_num'] = pd.to_numeric(df['block'].str.extract(r'(\\d+)', expand=False), errors='coerce').fillna(-1)\n",
        "        df['building_num'] = pd.to_numeric(df['buildingNumber'].astype(str).str.extract(r'(\\d+)', expand=False), errors='coerce').fillna(-1) * 0.1\n",
        "        df['floor_num'] = pd.to_numeric(df['floor'].astype(str).str.extract(r'(\\d+)', expand=False), errors='coerce').fillna(-1)\n",
        "        df['has_block'] = (df['block_num'] >= 0).astype(int)\n",
        "        df['has_building'] = (df['building_num'] >= 0).astype(int)\n",
        "        df['has_apartment'] = df['apartment'].notna().astype(int)\n",
        "        df['has_floor'] = (df['floor_num'] >= 0).astype(int)\n",
        "        df['has_street_num'] = df['street'].str.extract(r'(\\d+)$').notna().astype(int)\n",
        "        df['street_type'] = df['street'].apply(self.categorize_street)\n",
        "        df['area_block'] = df['area_normalized'].astype(str) + '_' + df['block'].astype(str)\n",
        "        df['block_street'] = df['block'].astype(str) + '_' + df['street'].astype(str)\n",
        "        geo_stats = self.artifacts['geo_stats']\n",
        "        for col in ['country', 'area', 'city', 'governorate', 'area_block', 'block_street']:\n",
        "            df[col] = df[col].fillna('unknown')\n",
        "            col_lat_mean = geo_stats.get(f'{col}_lat_mean', {}).get('mean', {})\n",
        "            col_lon_mean = geo_stats.get(f'{col}_lon_mean', {}).get('mean', {})\n",
        "            col_lat_std = geo_stats.get(f'{col}_lat_std', {}).get('std', {})\n",
        "            col_lon_std = geo_stats.get(f'{col}_lon_std', {}).get('std', {})\n",
        "            df[f'{col}_lat_mean'] = df[col].map(col_lat_mean).fillna(\n",
        "                df['governorate'].map(geo_stats.get('governorate_lat_mean', {}).get('mean', {})).fillna(self.kuwait_center['latitude'])\n",
        "            )\n",
        "            df[f'{col}_lon_mean'] = df[col].map(col_lon_mean).fillna(\n",
        "                df['governorate'].map(geo_stats.get('governorate_lon_mean', {}).get('mean', {})).fillna(self.kuwait_center['longitude'])\n",
        "            )\n",
        "            df[f'{col}_lat_std'] = df[col].map(col_lat_std).fillna(0.01)\n",
        "            df[f'{col}_lon_std'] = df[col].map(col_lon_std).fillna(0.01)\n",
        "        tfidf_features = self.artifacts['tfidf_vectorizer'].transform(df['input_text'].fillna(\"\"))\n",
        "        tfidf_df = pd.DataFrame(tfidf_features.toarray(), columns=[f'tfidf_{i}' for i in range(tfidf_features.shape[1])], index=df.index)\n",
        "        df = pd.concat([df, tfidf_df], axis=1)\n",
        "        address_embeddings = self.artifacts['sentence_embedder'].encode(df['input_text'].tolist(), batch_size=64, show_progress_bar=False)\n",
        "        manual_feature_cols = self.artifacts['manual_feature_columns']\n",
        "        for col in manual_feature_cols:\n",
        "            if col not in df.columns:\n",
        "                if 'tfidf_' in col:\n",
        "                    df[col] = 0.0\n",
        "                elif 'lat_mean' in col:\n",
        "                    df[col] = self.kuwait_center['latitude']\n",
        "                elif 'lon_mean' in col:\n",
        "                    df[col] = self.kuwait_center['longitude']\n",
        "                elif '_std' in col:\n",
        "                    df[col] = 0.01\n",
        "                else:\n",
        "                    df[col] = 0\n",
        "        manual_features = df[manual_feature_cols].fillna(0).values\n",
        "        X = np.hstack([address_embeddings, manual_features])\n",
        "        return X, df\n",
        "\n",
        "    def predict_coordinates_hybrid(self, addresses: List[str]) -> List[Dict]:\n",
        "        if not self.is_loaded:\n",
        "            self.load_artifacts()\n",
        "        try:\n",
        "            X, df = self.create_features_with_proper_fallbacks(addresses)\n",
        "            X_scaled = self.artifacts['feature_scaler'].transform(X)\n",
        "            predictions = self.artifacts['hybrid_model'].predict(X_scaled)\n",
        "            results = []\n",
        "            for i, (address, (lat, lon)) in enumerate(zip(addresses, predictions)):\n",
        "                is_valid = self.validate_coordinates(lat, lon)\n",
        "                area_similarity = df.iloc[i]['area_similarity']\n",
        "                has_block = df.iloc[i]['has_block']\n",
        "                has_street_num = df.iloc[i]['has_street_num']\n",
        "                is_named_street = df.iloc[i]['street_type'] == 'named'\n",
        "                area_penalty = 0.8 if df.iloc[i]['area'] == 'unknown' else 1.0\n",
        "                street_penalty = 0.8 if df.iloc[i]['street'] == 'unknown' else 0.9 if is_named_street else 1.0\n",
        "                confidence = (area_similarity * 0.4 + 0.4 * has_block + 0.2 * (has_street_num or is_named_street)) * area_penalty * street_penalty\n",
        "                status = \"hybrid_predicted\"\n",
        "                area_lat_mean = df.iloc[i]['area_lat_mean']\n",
        "                area_lon_mean = df.iloc[i]['area_lon_mean']\n",
        "                area_lat_std = df.iloc[i]['area_lat_std']\n",
        "                area_lon_std = df.iloc[i]['area_lon_std']\n",
        "                lat_deviation = abs(lat - area_lat_mean) / area_lat_std if area_lat_std > 0 else 0\n",
        "                lon_deviation = abs(lon - area_lon_mean) / area_lon_std if area_lon_std > 0 else 0\n",
        "                deviation_threshold = 3.0\n",
        "                if (not is_valid or confidence < 0.5 or lat_deviation > deviation_threshold or lon_deviation > deviation_threshold):\n",
        "                    lat = area_lat_mean\n",
        "                    lon = area_lon_mean\n",
        "                    if self.validate_coordinates(lat, lon):\n",
        "                        status = \"area_fallback\"\n",
        "                        confidence = 0.4\n",
        "                    else:\n",
        "                        lat = df.iloc[i]['governorate_lat_mean']\n",
        "                        lon = df.iloc[i]['governorate_lon_mean']\n",
        "                        status = \"governorate_fallback\"\n",
        "                        confidence = 0.2\n",
        "                confidence_str = \"high\" if confidence >= 0.7 else \"medium\" if confidence >= 0.4 else \"low\"\n",
        "                results.append({\n",
        "                    'input': address,\n",
        "                    'parsed_area': df.iloc[i]['area'],\n",
        "                    'parsed_block': df.iloc[i]['block'],\n",
        "                    'parsed_street': df.iloc[i]['street'],\n",
        "                    'parsed_buildingNumber': df.iloc[i]['buildingNumber'],\n",
        "                    'parsed_governorate': df.iloc[i]['governorate'],\n",
        "                    'latitude': float(lat),\n",
        "                    'longitude': float(lon),\n",
        "                    'status': status,\n",
        "                    'confidence': confidence_str\n",
        "                })\n",
        "            return results\n",
        "        except Exception as e:\n",
        "            return [{\n",
        "                'input': address,\n",
        "                'parsed_area': 'unknown',\n",
        "                'parsed_block': 'unknown',\n",
        "                'parsed_street': 'unknown',\n",
        "                'parsed_buildingNumber': '',\n",
        "                'parsed_governorate': 'unknown',\n",
        "                'latitude': self.kuwait_center['latitude'],\n",
        "                'longitude': self.kuwait_center['longitude'],\n",
        "                'status': 'error_fallback',\n",
        "                'confidence': 'low',\n",
        "                'error': str(e)\n",
        "            } for address in addresses]\n",
        "\n",
        "    def validate_coordinates(self, lat: float, lon: float) -> bool:\n",
        "        try:\n",
        "            lat, lon = float(lat), float(lon)\n",
        "            return (self.kuwait_bounds['lat_min'] - 0.02 <= lat <= self.kuwait_bounds['lat_max'] + 0.02 and\n",
        "                    self.kuwait_bounds['lon_min'] - 0.02 <= lon <= self.kuwait_bounds['lon_max'] + 0.02)\n",
        "        except (TypeError, ValueError):\n",
        "            return False\n",
        "\n",
        "def haversine_distance(lat1: float, lon1: float, lat2: float, lon2: float) -> float:\n",
        "    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n",
        "    dlat, dlon = lat2 - lat1, lon2 - lon1\n",
        "    a = sin(dlat/2)**2 + cos(lat1)*cos(lat2)*sin(dlon/2)**2\n",
        "    return 6371000 * 2 * asin(sqrt(a))\n",
        "\n",
        "def test_fixed_hybrid_geocoder():\n",
        "    geocoder = FixedHybridGeocoder()\n",
        "    test_addresses = [\n",
        "        \"Salmiya, Block 1, Street 1\",\n",
        "        \"Mubarak Al-Kabeer, Block 2, St 34\",\n",
        "        \"Hawalli, Block 4, tunis street\",\n",
        "        \"Mishref, Block 4, Street 2\",\n",
        "        \"Salmiya, Block 12, Street 2\",\n",
        "        \"Hawalli, Beirut Street, Commercial Bank of Kuwait\",\n",
        "        \"Hawalli , Block 4 , tunis street, Hawalli Park\",\n",
        "        \"Salwa Block 11 street 10\",\n",
        "        \"Salmiya, Block 4, Street 2, building 14\",\n",
        "        \"Salmiya, Block 7, street 77,bldg 12\",\n",
        "        \"Salmiya, Block 10, Street 10, bldg 57\",\n",
        "        \"Salmiya, Block 10, bldg 57\",\n",
        "        \"Sabahiya, Block 4, 9 street,building 220\",\n",
        "        \"Zahra, Block 3, St 310,building 5\",\n",
        "        \"Rawda, Block 5, St 50, 12\",\n",
        "        \"Mubarak Al-Kabeer, Block 5, St 23, 9\"\n",
        "    ]\n",
        "    ground_truth = [\n",
        "        (29.3492824, 48.0953218),\n",
        "        (29.1924, 48.0774),\n",
        "        (29.341525, 48.019292),\n",
        "        (29.2779, 48.0690),\n",
        "        (29.3246, 48.0568),\n",
        "        (29.335272277402016, 48.016834684876756),\n",
        "        (29.34037499802647, 48.02221585684463),\n",
        "        (29.284326847832105, 48.08411563806048),\n",
        "        (29.341837, 48.081363),\n",
        "        (29.33749, 48.0638),\n",
        "        (29.327933375139658, 48.06915761997239),\n",
        "        (29.327933375139658, 48.06915761997239),\n",
        "        (29.113487, 48.112334),\n",
        "        (29.2803, 47.9899),\n",
        "        (29.3302,47.994),\n",
        "        (29.189831,48.085122)\n",
        "    ]\n",
        "    results = geocoder.predict_coordinates_hybrid(test_addresses)\n",
        "    for i, (result, gt) in enumerate(zip(results, ground_truth), 1):\n",
        "        distance = haversine_distance(result['latitude'], result['longitude'], gt[0], gt[1])\n",
        "        print(f\"{i:2d}. Input: {result['input']}\")\n",
        "        print(f\"    Coordinates: ({result['latitude']:.6f}, {result['longitude']:.6f})\")\n",
        "        print(f\"    Confidence: {result['confidence']}\")\n",
        "        print(f\"    Distance Error: {distance:.1f} m\")\n",
        "        if i < len(results):\n",
        "            print(\"\\n\" + \"=\"*60)\n",
        "    return results, []\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test_results, _ = test_fixed_hybrid_geocoder()\n"
      ],
      "metadata": {
        "id": "nyRZpTo1sq0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rapidfuzz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzDINUQB5_zV",
        "outputId": "758f78a3-dfc5-4249-e445-26785b7703f2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rapidfuzz in /usr/local/lib/python3.12/dist-packages (3.14.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jellyfish"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTcNDZ7Dsqyl",
        "outputId": "41604c29-e53e-44e4-b06c-efee9b79aee0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jellyfish\n",
            "  Downloading jellyfish-1.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.6 kB)\n",
            "Downloading jellyfish-1.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (355 kB)\n",
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/355.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.7/355.9 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m \u001b[32m348.2/355.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m355.9/355.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jellyfish\n",
            "Successfully installed jellyfish-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fWnJXCnE5jCD"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import faiss\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "import pandas as pd\n",
        "import joblib\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from rapidfuzz import fuzz, process as rapid_process\n",
        "import jellyfish\n",
        "from math import radians, sin, cos, asin, sqrt\n",
        "import xgboost as xgb\n",
        "from typing import List, Dict, Tuple\n",
        "\n",
        "# Define EnhancedXGBoostGeocoder class\n",
        "class EnhancedXGBoostGeocoder:\n",
        "    def __init__(self):\n",
        "        self.lat_model = None\n",
        "        self.lon_model = None\n",
        "\n",
        "    def train(self, X_train, y_train, X_val=None, y_val=None, weights_train=None):\n",
        "        print(\"Training Enhanced XGBoost Geocoder...\")\n",
        "        params = {\n",
        "            'objective': 'reg:squarederror',\n",
        "            'n_estimators': 800,\n",
        "            'max_depth': 6,\n",
        "            'learning_rate': 0.1,\n",
        "            'subsample': 0.8,\n",
        "            'colsample_bytree': 0.8,\n",
        "            'tree_method': 'hist',\n",
        "            'random_state': 42\n",
        "        }\n",
        "        self.lat_model = xgb.XGBRegressor(**params)\n",
        "        self.lon_model = xgb.XGBRegressor(**params)\n",
        "        self.lat_model.fit(X_train, y_train[:, 0], sample_weight=weights_train)\n",
        "        self.lon_model.fit(X_train, y_train[:, 1], sample_weight=weights_train)\n",
        "        if X_val is not None and y_val is not None:\n",
        "            print(\"Evaluating on validation set...\")\n",
        "            val_pred = self.predict(X_val)\n",
        "            from sklearn.metrics import mean_squared_error\n",
        "            mse_lat = mean_squared_error(y_val[:, 0], val_pred[:, 0])\n",
        "            mse_lon = mean_squared_error(y_val[:, 1], val_pred[:, 1])\n",
        "            print(f\"Validation MSE - Latitude: {mse_lat:.6f}, Longitude: {mse_lon:.6f}\")\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        lat_pred = self.lat_model.predict(X)\n",
        "        lon_pred = self.lon_model.predict(X)\n",
        "        return np.column_stack((lat_pred, lon_pred))\n",
        "\n",
        "# Define HybridXGBFAISSGeocoder class\n",
        "class HybridXGBFAISSGeocoder:\n",
        "    def __init__(self, k_neighbors=3, models_dir=\"models\"):\n",
        "        self.xgb_geocoder = EnhancedXGBoostGeocoder()\n",
        "        self.faiss_lat_index = None\n",
        "        self.faiss_lon_index = None\n",
        "        self.residuals = None\n",
        "        self.k = k_neighbors\n",
        "        self.X_train_ref = None\n",
        "        self.weights_train = None\n",
        "        self.distance_weighting = 'inverse_squared'\n",
        "        self.residual_cap = 0.01\n",
        "        self.models_dir = models_dir\n",
        "\n",
        "    def train(self, X_train, y_train, X_val=None, y_val=None, weights_train=None):\n",
        "        print(\"ðŸ”¥ Training Hybrid XGBoost + FAISS model...\")\n",
        "        self.xgb_geocoder.train(X_train, y_train, X_val, y_val, weights_train=weights_train)\n",
        "        xgb_pred = self.xgb_geocoder.predict(X_train)\n",
        "        self.residuals = y_train - xgb_pred\n",
        "        self.weights_train = weights_train\n",
        "        d = X_train.shape[1]\n",
        "        X_train_f32 = X_train.astype(np.float32)\n",
        "        self.faiss_lat_index = faiss.IndexFlatL2(d)\n",
        "        self.faiss_lon_index = faiss.IndexFlatL2(d)\n",
        "        self.faiss_lat_index.add(X_train_f32)\n",
        "        self.faiss_lon_index.add(X_train_f32)\n",
        "        self.X_train_ref = X_train_f32\n",
        "        os.makedirs(self.models_dir, exist_ok=True)\n",
        "        faiss.write_index(self.faiss_lat_index, os.path.join(self.models_dir, 'faiss_lat_index.index'))\n",
        "        faiss.write_index(self.faiss_lon_index, os.path.join(self.models_dir, 'faiss_lon_index.index'))\n",
        "        np.save(os.path.join(self.models_dir, 'X_train_ref.npy'), X_train_f32)\n",
        "        np.save(os.path.join(self.models_dir, 'residuals.npy'), self.residuals)\n",
        "        if self.weights_train is not None:\n",
        "            np.save(os.path.join(self.models_dir, 'sample_weights.npy'), self.weights_train)\n",
        "        print(f\"âœ… FAISS indexes built with {len(X_train)} reference points\")\n",
        "        return self\n",
        "\n",
        "    def load_artifacts(self):\n",
        "        self.faiss_lat_index = faiss.read_index(os.path.join(self.models_dir, 'faiss_lat_index.index'))\n",
        "        self.faiss_lon_index = faiss.read_index(os.path.join(self.models_dir, 'faiss_lon_index.index'))\n",
        "        self.X_train_ref = np.load(os.path.join(self.models_dir, 'X_train_ref.npy'))\n",
        "        self.residuals = np.load(os.path.join(self.models_dir, 'residuals.npy'))\n",
        "        weights_path = os.path.join(self.models_dir, 'sample_weights.npy')\n",
        "        if os.path.exists(weights_path):\n",
        "            self.weights_train = np.load(weights_path)\n",
        "        else:\n",
        "            self.weights_train = None\n",
        "\n",
        "    def predict(self, X):\n",
        "        if self.xgb_geocoder.lat_model is None or self.xgb_geocoder.lon_model is None:\n",
        "            raise ValueError(\"Models not trained yet. Call train() first.\")\n",
        "        if self.faiss_lat_index is None:\n",
        "            self.load_artifacts()\n",
        "        xgb_pred = self.xgb_geocoder.predict(X)\n",
        "        X_f32 = X.astype(np.float32)\n",
        "        dist_lat, idx_lat = self.faiss_lat_index.search(X_f32, self.k)\n",
        "        dist_lon, idx_lon = self.faiss_lon_index.search(X_f32, self.k)\n",
        "        eps = 1e-8\n",
        "        weights_lat = 1 / (dist_lat + eps)\n",
        "        weights_lon = 1 / (dist_lon + eps)\n",
        "        weights_lat /= weights_lat.sum(axis=1, keepdims=True)\n",
        "        weights_lon /= weights_lon.sum(axis=1, keepdims=True)\n",
        "        if self.weights_train is not None:\n",
        "            weights_lat = weights_lat * self.weights_train[idx_lat]\n",
        "            weights_lon = weights_lon * self.weights_train[idx_lon]\n",
        "            weights_lat /= weights_lat.sum(axis=1, keepdims=True)\n",
        "            weights_lon /= weights_lon.sum(axis=1, keepdims=True)\n",
        "        correction_lat = (weights_lat * self.residuals[idx_lat, 0]).sum(axis=1)\n",
        "        correction_lon = (weights_lon * self.residuals[idx_lon, 1]).sum(axis=1)\n",
        "        final_pred = xgb_pred + np.column_stack((correction_lat, correction_lon))\n",
        "        return final_pred\n",
        "\n",
        "# Define FixedHybridGeocoder class\n",
        "class FixedHybridGeocoder:\n",
        "    def __init__(self, models_dir: str = \"models\"):\n",
        "        self.models_dir = models_dir\n",
        "        self.artifacts = {}\n",
        "        self.is_loaded = False\n",
        "        self.kuwait_governorates = {}\n",
        "        self.abbreviation_map = {}\n",
        "        self.common_typos = {}\n",
        "        self.all_kuwait_areas = []\n",
        "        self.typo_patterns = []\n",
        "        self.kuwait_bounds = {\n",
        "            'lat_min': 28.524574,\n",
        "            'lat_max': 30.103532,\n",
        "            'lon_min': 46.552695,\n",
        "            'lon_max': 48.416094\n",
        "        }\n",
        "        self.kuwait_center = {\n",
        "            'latitude': 29.3759,\n",
        "            'longitude': 47.9774\n",
        "        }\n",
        "        self.area_similarity_cache = {}\n",
        "        self.embedding_cache = {}\n",
        "        self.load_artifacts()\n",
        "\n",
        "    def load_artifacts(self):\n",
        "        try:\n",
        "            self.artifacts['feature_scaler'] = joblib.load(os.path.join(self.models_dir, 'feature_scaler.pkl'))\n",
        "            self.artifacts['tfidf_vectorizer'] = joblib.load(os.path.join(self.models_dir, 'tfidf_vectorizer.pkl'))\n",
        "            self.artifacts['manual_feature_columns'] = joblib.load(os.path.join(self.models_dir, 'manual_feature_columns.pkl'))\n",
        "            # Load sentence embedder from models directory\n",
        "            sentence_embedder_path = os.path.join(self.models_dir, 'sentence_embedder')\n",
        "            if os.path.exists(sentence_embedder_path):\n",
        "                self.artifacts['sentence_embedder'] = SentenceTransformer(sentence_embedder_path)\n",
        "            else:\n",
        "                raise FileNotFoundError(f\"Sentence embedder not found in {sentence_embedder_path}\")\n",
        "            self.artifacts['geo_stats'] = joblib.load(os.path.join(self.models_dir, 'geo_stats.pkl'))\n",
        "            with open(os.path.join(self.models_dir, 'address_normalization_dicts.json'), 'r', encoding='utf-8') as f:\n",
        "                address_dicts = json.load(f)\n",
        "            self.kuwait_governorates = address_dicts['kuwait_governorates']\n",
        "            self.abbreviation_map = address_dicts['abbreviation_map']\n",
        "            self.common_typos = address_dicts['common_typos']\n",
        "            if 'sharq' in self.abbreviation_map:\n",
        "                del self.abbreviation_map['sharq']\n",
        "            if 'sharq' in self.common_typos:\n",
        "                self.common_typos['sharq'] = 'sharq'\n",
        "            self.common_typos = {k: v for k, v in self.common_typos.items() if len(k) > 1}\n",
        "            self.all_kuwait_areas = [area for gov_areas in self.kuwait_governorates.values() for area in gov_areas]\n",
        "            self.all_kuwait_areas.extend([correct for typo, correct in self.common_typos.items() if correct not in self.all_kuwait_areas])\n",
        "            self.all_kuwait_areas = list(set(self.normalize_text(area) for area in self.all_kuwait_areas))\n",
        "            self.typo_patterns = [(re.compile(rf\"\\\\b{re.escape(typo)}\\\\b\", re.IGNORECASE), correct) for typo, correct in sorted(self.common_typos.items(), key=lambda x: len(x[0]), reverse=True)]\n",
        "            with open(os.path.join(self.models_dir, 'training_metadata.json'), 'r') as f:\n",
        "                self.artifacts['metadata'] = json.load(f)\n",
        "            self.artifacts['hybrid_model'] = joblib.load(os.path.join(self.models_dir, 'hybrid_xgbfaiss_geocoder.pkl'))\n",
        "            self._validate_loaded_components()\n",
        "            self.is_loaded = True\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Failed to load artifacts: {e}\")\n",
        "\n",
        "    def _validate_loaded_components(self):\n",
        "        expected_dims = self.artifacts['metadata']['feature_dimensions']\n",
        "        actual_dims = self.artifacts['hybrid_model'].xgb_geocoder.lat_model.n_features_in_\n",
        "        if expected_dims != actual_dims:\n",
        "            raise ValueError(f\"Feature dimension mismatch: expected {expected_dims}, got {actual_dims}\")\n",
        "\n",
        "    def normalize_text(self, text: str) -> str:\n",
        "        if not text or pd.isna(text):\n",
        "            return \"\"\n",
        "        text = str(text).strip().lower()\n",
        "        translation_table = str.maketrans(\"Ù Ù¡Ù¢Ù£Ù¤Ù¥Ù¦Ù§Ù¨Ù©Ø¥Ø£Ø¢Ø§Ù‰Ø¦Ø¡Ø¤\", \"0123456789Ø§Ø§Ø§Ø§ÙŠØ¡Ø¡Ø¡\")\n",
        "        text = text.translate(translation_table)\n",
        "        text = re.sub(r\"[^\\w\\s\\d\\u0600-\\u06FF]\", \" \", text)\n",
        "        words = text.split()\n",
        "        text = \" \".join(self.abbreviation_map.get(word, self.common_typos.get(word, word)) for word in words)\n",
        "        return text\n",
        "\n",
        "    def validate_kuwaiti_block(self, block: str) -> bool:\n",
        "        if not block or pd.isna(block):\n",
        "            return False\n",
        "        return bool(re.fullmatch(r\"^\\d{1,3}[a-zA-Z]?$\", str(block).strip()))\n",
        "\n",
        "    def validate_kuwaiti_street(self, street: str) -> bool:\n",
        "        if not street or pd.isna(street):\n",
        "            return False\n",
        "        street = self.normalize_text(street)\n",
        "        return bool(re.fullmatch(\n",
        "            r\"^(?:street|avenue|road|lane|crescent|Ø´Ø§Ø±Ø¹|Ø¬Ø§Ø¯Ø©|Ø·Ø±ÙŠÙ‚|Ø­Ø§Ø±Ø©|Ù‡Ù„Ø§Ù„)?\\s*[\\d\\w\\s\\-]+$\",\n",
        "            street, re.IGNORECASE\n",
        "        )) and len(street) <= 100\n",
        "\n",
        "    def categorize_street(self, street: str) -> str:\n",
        "        street = self.normalize_text(street)\n",
        "        if street == \"unknown\" or not street:\n",
        "            return \"unknown\"\n",
        "        if re.match(r\"^(?:street|avenue|road|lane|crescent|Ø´Ø§Ø±Ø¹|Ø¬Ø§Ø¯Ø©|Ø·Ø±ÙŠÙ‚|Ø­Ø§Ø±Ø©|Ù‡Ù„Ø§Ù„)?\\s*\\d+$\", street, re.IGNORECASE):\n",
        "            return \"numbered\"\n",
        "        if re.match(r\"^(?:street|avenue|road|lane|crescent|Ø´Ø§Ø±Ø¹|Ø¬Ø§Ø¯Ø©|Ø·Ø±ÙŠÙ‚|Ø­Ø§Ø±Ø©|Ù‡Ù„Ø§Ù„)?\\s*[\\w\\s\\-]+$\", street, re.IGNORECASE):\n",
        "            return \"named\"\n",
        "        return \"unknown\"\n",
        "\n",
        "    def parse_address_robust(self, address: str) -> Dict:\n",
        "        if not address or not isinstance(address, str):\n",
        "            return {\n",
        "                'country': 'kuwait', 'area': 'unknown', 'block': 'unknown', 'street': 'unknown',\n",
        "                'buildingNumber': '', 'apartment': '', 'floor': ''\n",
        "            }\n",
        "        normalized_address = self.normalize_text(address)\n",
        "        result = {\n",
        "            'country': 'kuwait', 'area': 'unknown', 'block': 'unknown', 'street': 'unknown',\n",
        "            'buildingNumber': '', 'apartment': '', 'floor': ''\n",
        "        }\n",
        "        area_found = self.extract_area_advanced(normalized_address)\n",
        "        if area_found != 'unknown':\n",
        "            result['area'] = area_found\n",
        "        block_found = self.extract_block_robust(normalized_address)\n",
        "        if block_found != 'unknown':\n",
        "            result['block'] = block_found\n",
        "        street_found = self.extract_street_robust(normalized_address)\n",
        "        if street_found != 'unknown':\n",
        "            result['street'] = street_found\n",
        "        result.update(self.extract_other_components(normalized_address))\n",
        "        return result\n",
        "\n",
        "    def extract_area_advanced(self, text: str) -> str:\n",
        "        text = self.normalize_text(text)\n",
        "        text_words = set(text.split())\n",
        "        candidates = []\n",
        "        phonetic_areas = {jellyfish.nysiis(area): area for area in self.all_kuwait_areas}\n",
        "        text_phonetic = jellyfish.nysiis(text)\n",
        "        for area in self.all_kuwait_areas:\n",
        "            area_norm = self.normalize_text(area)\n",
        "            area_words = set(area_norm.split())\n",
        "            if not area_words:\n",
        "                continue\n",
        "            score = len(text_words & area_words) / len(area_words)\n",
        "            position = text.find(area_norm)\n",
        "            if score >= 0.5 and position != -1:\n",
        "                penalty = 0.1 if len(area_words) > 1 else 0.0\n",
        "                candidates.append((area_norm, score - penalty, position, len(area_words)))\n",
        "        if text_phonetic in phonetic_areas:\n",
        "            phonetic_match = phonetic_areas[text_phonetic]\n",
        "            candidates.append((phonetic_match, 0.9, 0, 1))\n",
        "        best_match = 'unknown'\n",
        "        if candidates:\n",
        "            candidates.sort(key=lambda x: (-x[1], x[2], x[3]))\n",
        "            best_match = candidates[0][0]\n",
        "        if best_match == 'unknown':\n",
        "            kuwait_city_norm = self.normalize_text('kuwait city')\n",
        "            if kuwait_city_norm in text and 'sharq' not in text and 'mubarak' not in text.lower():\n",
        "                best_match = kuwait_city_norm\n",
        "        if best_match == 'unknown':\n",
        "            matches = rapid_process.extract(text, self.all_kuwait_areas, scorer=fuzz.token_sort_ratio, limit=1)\n",
        "            if matches and matches[0][1] >= 70:\n",
        "                best_match = self.normalize_text(matches[0][0])\n",
        "        return best_match\n",
        "\n",
        "    def extract_block_robust(self, text: str) -> str:\n",
        "        text = self.normalize_text(text)\n",
        "        patterns = [\n",
        "            r'block\\s+(\\d{1,3}[a-zA-Z]?)',\n",
        "            r'blk\\s+(\\d{1,3}[a-zA-Z]?)',\n",
        "            r'b\\s+(\\d{1,3}[a-zA-Z]?)',\n",
        "            r'(\\d{1,3}[a-zA-Z]?)\\s*(?:street|st|avenue|ave|road|rd|lane|ln|crescent|cr|Ø´Ø§Ø±Ø¹|Ø¬Ø§Ø¯Ø©|Ø·Ø±ÙŠÙ‚|Ø­Ø§Ø±Ø©|Ù‡Ù„Ø§Ù„)'\n",
        "        ]\n",
        "        for pattern in patterns:\n",
        "            match = re.search(pattern, text, re.IGNORECASE)\n",
        "            if match and self.validate_kuwaiti_block(match.group(1)):\n",
        "                return match.group(1)\n",
        "        return 'unknown'\n",
        "\n",
        "    def extract_street_robust(self, text: str) -> str:\n",
        "        text = self.normalize_text(text)\n",
        "        patterns = [\n",
        "            r'(?:street|avenue|road|lane|crescent|Ø´Ø§Ø±Ø¹|Ø¬Ø§Ø¯Ø©|Ø·Ø±ÙŠÙ‚|Ø­Ø§Ø±Ø©|Ù‡Ù„Ø§Ù„)\\s+([\\w\\s\\-]+?)(?=\\s*(?:block|building\\s+\\d+|floor|apartment|apt|\\d+\\s*$|$))',\n",
        "            r'(?:st|ave|rd|ln|cr)\\s+([\\w\\s\\-]+?)(?=\\s*(?:block|building\\s+\\d+|floor|apartment|apt|\\d+\\s*$|$))',\n",
        "            r'([\\w\\s\\-]+?)\\s+(?:street|avenue|road|lane|crescent|st|ave|rd|ln|cr|Ø´Ø§Ø±Ø¹|Ø¬Ø§Ø¯Ø©|Ø·Ø±ÙŠÙ‚|Ø­Ø§Ø±Ø©|Ù‡Ù„Ø§Ù„)(?=\\s*(?:block|building\\s+\\d+|floor|apartment|apt|\\d+\\s*$|$))'\n",
        "        ]\n",
        "        for pattern in patterns:\n",
        "            match = re.search(pattern, text, re.IGNORECASE)\n",
        "            if match:\n",
        "                street = match.group(1).strip()\n",
        "                if (self.validate_kuwaiti_street(street) and\n",
        "                    not any(area in self.normalize_text(street) for area in self.all_kuwait_areas) and\n",
        "                    not any(kw in street.lower() for kw in ['block', 'building', 'floor', 'apartment', 'apt'])):\n",
        "                    return street\n",
        "        words = text.split()\n",
        "        street_keywords = ['street', 'avenue', 'road', 'lane', 'crescent', 'Ø´Ø§Ø±Ø¹', 'Ø¬Ø§Ø¯Ø©', 'Ø·Ø±ÙŠÙ‚', 'Ø­Ø§Ø±Ø©', 'Ù‡Ù„Ø§Ù„', 'st', 'ave', 'rd', 'ln', 'cr']\n",
        "        for i, word in enumerate(words):\n",
        "            if word.lower() in street_keywords:\n",
        "                street_words_after = []\n",
        "                for j in range(i + 1, len(words)):\n",
        "                    next_word = words[j].lower()\n",
        "                    if (next_word in ['block', 'building', 'floor', 'apartment', 'apt'] or\n",
        "                        re.match(r'^\\d+$', next_word) or\n",
        "                        self.normalize_text(next_word) in self.all_kuwait_areas):\n",
        "                        break\n",
        "                    street_words_after.append(words[j])\n",
        "                street_words_before = []\n",
        "                for j in range(i - 1, -1, -1):\n",
        "                    prev_word = words[j].lower()\n",
        "                    if j - 1 >= 0 and words[j - 1].lower() == 'block' and bool(re.fullmatch(r\"^\\d{1,3}[a-zA-Z]?$\", prev_word)):\n",
        "                        break\n",
        "                    if (prev_word in ['block', 'building', 'floor', 'apartment', 'apt'] or\n",
        "                        self.normalize_text(prev_word) in self.all_kuwait_areas):\n",
        "                        break\n",
        "                    street_words_before.insert(0, words[j])\n",
        "                street = ' '.join(street_words_before + street_words_after).strip()\n",
        "                if (street and self.validate_kuwaiti_street(street) and\n",
        "                    not any(area in self.normalize_text(street) for area in self.all_kuwait_areas) and\n",
        "                    not any(kw in street.lower() for kw in ['block', 'building', 'floor', 'apartment', 'apt'])):\n",
        "                    return street\n",
        "        return 'unknown'\n",
        "\n",
        "    def extract_other_components(self, text: str) -> Dict:\n",
        "        components = {}\n",
        "        text = self.normalize_text(text)\n",
        "        building_match = re.search(r'building\\s+(\\d+)|(\\d+)\\s*$', text, re.IGNORECASE)\n",
        "        if building_match:\n",
        "            components['buildingNumber'] = building_match.group(1) or building_match.group(2)\n",
        "        floor_match = re.search(r'floor\\s+(\\d+)', text, re.IGNORECASE)\n",
        "        if floor_match:\n",
        "            components['floor'] = floor_match.group(1)\n",
        "        apt_match = re.search(r'(?:apt|apartment)\\s+(\\w+)', text, re.IGNORECASE)\n",
        "        if apt_match:\n",
        "            components['apartment'] = apt_match.group(1)\n",
        "        return components\n",
        "\n",
        "    def get_governorate(self, area: str) -> str:\n",
        "        area_norm = self.normalize_text(area)\n",
        "        for gov, areas in self.kuwait_governorates.items():\n",
        "            if any(area_norm == self.normalize_text(a) for a in areas):\n",
        "                return gov\n",
        "        return \"unknown\"\n",
        "\n",
        "    def create_features_with_proper_fallbacks(self, addresses: List[str]) -> Tuple[np.ndarray, pd.DataFrame]:\n",
        "        parsed_data = []\n",
        "        for address in addresses:\n",
        "            parsed = self.parse_address_robust(address)\n",
        "            normalized_address = self.normalize_text(address)\n",
        "            normalized_address = re.sub(r'\\bbuilding\\s+\\d+\\b', '', normalized_address, flags=re.IGNORECASE).strip()\n",
        "            parsed['input_text'] = normalized_address\n",
        "            parsed['governorate'] = self.get_governorate(parsed['area'])\n",
        "            parsed['city'] = parsed['area']\n",
        "            parsed['area_normalized'] = self.normalize_text(parsed['area'])\n",
        "            area_key = (parsed['area_normalized'], tuple(self.all_kuwait_areas))\n",
        "            if area_key in self.area_similarity_cache:\n",
        "                parsed['area_similarity'] = self.area_similarity_cache[area_key]\n",
        "            else:\n",
        "                parsed['area_similarity'] = rapid_process.extractOne(\n",
        "                    parsed['area_normalized'], self.all_kuwait_areas, scorer=fuzz.token_sort_ratio\n",
        "                )[1] / 100.0 if parsed['area'] != 'unknown' else 0.0\n",
        "                self.area_similarity_cache[area_key] = parsed['area_similarity']\n",
        "            parsed_data.append(parsed)\n",
        "        df = pd.DataFrame(parsed_data)\n",
        "        df['block_num'] = pd.to_numeric(df['block'].str.extract(r'(\\d+)', expand=False), errors='coerce').fillna(-1)\n",
        "        df['building_num'] = pd.to_numeric(df['buildingNumber'].astype(str).str.extract(r'(\\d+)', expand=False), errors='coerce').fillna(-1) * 0.1\n",
        "        df['floor_num'] = pd.to_numeric(df['floor'].astype(str).str.extract(r'(\\d+)', expand=False), errors='coerce').fillna(-1)\n",
        "        df['has_block'] = (df['block_num'] >= 0).astype(int)\n",
        "        df['has_building'] = (df['building_num'] >= 0).astype(int)\n",
        "        df['has_apartment'] = df['apartment'].notna().astype(int)\n",
        "        df['has_floor'] = (df['floor_num'] >= 0).astype(int)\n",
        "        df['has_street_num'] = df['street'].str.extract(r'(\\d+)$').notna().astype(int)\n",
        "        df['street_type'] = df['street'].apply(self.categorize_street)\n",
        "        df['area_block'] = df['area_normalized'].astype(str) + '_' + df['block'].astype(str)\n",
        "        df['block_street'] = df['block'].astype(str) + '_' + df['street'].astype(str)\n",
        "        geo_stats = self.artifacts['geo_stats']\n",
        "        for col in ['country', 'area', 'city', 'governorate', 'area_block', 'block_street']:\n",
        "            df[col] = df[col].fillna('unknown')\n",
        "            col_lat_mean = geo_stats.get(f'{col}_lat_mean', {}).get('mean', {})\n",
        "            col_lon_mean = geo_stats.get(f'{col}_lon_mean', {}).get('mean', {})\n",
        "            col_lat_std = geo_stats.get(f'{col}_lat_std', {}).get('std', {})\n",
        "            col_lon_std = geo_stats.get(f'{col}_lon_std', {}).get('std', {})\n",
        "            df[f'{col}_lat_mean'] = df[col].map(col_lat_mean).fillna(\n",
        "                df['governorate'].map(geo_stats.get('governorate_lat_mean', {}).get('mean', {})).fillna(self.kuwait_center['latitude'])\n",
        "            )\n",
        "            df[f'{col}_lon_mean'] = df[col].map(col_lon_mean).fillna(\n",
        "                df['governorate'].map(geo_stats.get('governorate_lon_mean', {}).get('mean', {})).fillna(self.kuwait_center['longitude'])\n",
        "            )\n",
        "            df[f'{col}_lat_std'] = df[col].map(col_lat_std).fillna(0.01)\n",
        "            df[f'{col}_lon_std'] = df[col].map(col_lon_std).fillna(0.01)\n",
        "        tfidf_features = self.artifacts['tfidf_vectorizer'].transform(df['input_text'].fillna(\"\"))\n",
        "        tfidf_df = pd.DataFrame(tfidf_features.toarray(), columns=[f'tfidf_{i}' for i in range(tfidf_features.shape[1])], index=df.index)\n",
        "        df = pd.concat([df, tfidf_df], axis=1)\n",
        "        address_embeddings = []\n",
        "        texts_to_encode = []\n",
        "        indices_to_encode = []\n",
        "        for i, text in enumerate(df['input_text'].tolist()):\n",
        "            if text in self.embedding_cache:\n",
        "                address_embeddings.append(self.embedding_cache[text])\n",
        "            else:\n",
        "                texts_to_encode.append(text)\n",
        "                indices_to_encode.append(i)\n",
        "        if texts_to_encode:\n",
        "            new_embeddings = self.artifacts['sentence_embedder'].encode(\n",
        "                texts_to_encode, batch_size=64, show_progress_bar=False\n",
        "            )\n",
        "            for idx, text, emb in zip(indices_to_encode, texts_to_encode, new_embeddings):\n",
        "                self.embedding_cache[text] = emb\n",
        "                address_embeddings.insert(idx, emb)\n",
        "        address_embeddings = np.array(address_embeddings)\n",
        "        manual_feature_cols = self.artifacts['manual_feature_columns']\n",
        "        for col in manual_feature_cols:\n",
        "            if col not in df.columns:\n",
        "                if 'tfidf_' in col:\n",
        "                    df[col] = 0.0\n",
        "                elif 'lat_mean' in col:\n",
        "                    df[col] = self.kuwait_center['latitude']\n",
        "                elif 'lon_mean' in col:\n",
        "                    df[col] = self.kuwait_center['longitude']\n",
        "                elif '_std' in col:\n",
        "                    df[col] = 0.01\n",
        "                else:\n",
        "                    df[col] = 0\n",
        "        manual_features = df[manual_feature_cols].fillna(0).values\n",
        "        X = np.hstack([address_embeddings, manual_features])\n",
        "        return X, df\n",
        "\n",
        "    def predict_coordinates_hybrid(self, addresses: List[str]) -> List[Dict]:\n",
        "        try:\n",
        "            X, df = self.create_features_with_proper_fallbacks(addresses)\n",
        "            X_scaled = self.artifacts['feature_scaler'].transform(X)\n",
        "            predictions = self.artifacts['hybrid_model'].predict(X_scaled)\n",
        "            results = []\n",
        "            for i, (address, (lat, lon)) in enumerate(zip(addresses, predictions)):\n",
        "                is_valid = self.validate_coordinates(lat, lon)\n",
        "                area_similarity = df.iloc[i]['area_similarity']\n",
        "                has_block = df.iloc[i]['has_block']\n",
        "                has_street_num = df.iloc[i]['has_street_num']\n",
        "                is_named_street = df.iloc[i]['street_type'] == 'named'\n",
        "                area_penalty = 0.8 if df.iloc[i]['area'] == 'unknown' else 1.0\n",
        "                street_penalty = 0.8 if df.iloc[i]['street'] == 'unknown' else 0.9 if is_named_street else 1.0\n",
        "                confidence = (area_similarity * 0.4 + 0.4 * has_block + 0.2 * (has_street_num or is_named_street)) * area_penalty * street_penalty\n",
        "                status = \"hybrid_predicted\"\n",
        "                area_lat_mean = df.iloc[i]['area_lat_mean']\n",
        "                area_lon_mean = df.iloc[i]['area_lon_mean']\n",
        "                area_lat_std = df.iloc[i]['area_lat_std']\n",
        "                area_lon_std = df.iloc[i]['area_lon_std']\n",
        "                lat_deviation = abs(lat - area_lat_mean) / area_lat_std if area_lat_std > 0 else 0\n",
        "                lon_deviation = abs(lon - area_lon_mean) / area_lon_std if area_lon_std > 0 else 0\n",
        "                deviation_threshold = 3.0\n",
        "                if (not is_valid or confidence < 0.5 or lat_deviation > deviation_threshold or lon_deviation > deviation_threshold):\n",
        "                    lat = area_lat_mean\n",
        "                    lon = area_lon_mean\n",
        "                    if self.validate_coordinates(lat, lon):\n",
        "                        status = \"area_fallback\"\n",
        "                        confidence = 0.4\n",
        "                    else:\n",
        "                        lat = df.iloc[i]['governorate_lat_mean']\n",
        "                        lon = df.iloc[i]['governorate_lon_mean']\n",
        "                        status = \"governorate_fallback\"\n",
        "                        confidence = 0.2\n",
        "                confidence_str = \"high\" if confidence >= 0.7 else \"medium\" if confidence >= 0.4 else \"low\"\n",
        "                results.append({\n",
        "                    'input': address,\n",
        "                    'parsed_area': df.iloc[i]['area'],\n",
        "                    'parsed_block': df.iloc[i]['block'],\n",
        "                    'parsed_street': df.iloc[i]['street'],\n",
        "                    'parsed_buildingNumber': df.iloc[i]['buildingNumber'],\n",
        "                    'parsed_governorate': df.iloc[i]['governorate'],\n",
        "                    'latitude': float(lat),\n",
        "                    'longitude': float(lon),\n",
        "                    'status': status,\n",
        "                    'confidence': confidence_str\n",
        "                })\n",
        "            return results\n",
        "        except Exception as e:\n",
        "            return [{\n",
        "                'input': address,\n",
        "                'parsed_area': 'unknown',\n",
        "                'parsed_block': 'unknown',\n",
        "                'parsed_street': 'unknown',\n",
        "                'parsed_buildingNumber': '',\n",
        "                'parsed_governorate': 'unknown',\n",
        "                'latitude': self.kuwait_center['latitude'],\n",
        "                'longitude': self.kuwait_center['longitude'],\n",
        "                'status': 'error_fallback',\n",
        "                'confidence': 'low',\n",
        "                'error': str(e)\n",
        "            } for address in addresses]\n",
        "\n",
        "    def validate_coordinates(self, lat: float, lon: float) -> bool:\n",
        "        try:\n",
        "            lat, lon = float(lat), float(lon)\n",
        "            return (self.kuwait_bounds['lat_min'] - 0.02 <= lat <= self.kuwait_bounds['lat_max'] + 0.02 and\n",
        "                    self.kuwait_bounds['lon_min'] - 0.02 <= lon <= self.kuwait_bounds['lon_max'] + 0.02)\n",
        "        except (TypeError, ValueError):\n",
        "            return False\n",
        "\n",
        "def haversine_distance(lat1: np.ndarray, lon1: np.ndarray, lat2: np.ndarray, lon2: np.ndarray) -> np.ndarray:\n",
        "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
        "    dlat, dlon = lat2 - lat1, lon2 - lon1\n",
        "    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
        "    return 6371000 * 2 * np.arcsin(np.sqrt(a))\n",
        "\n",
        "def test_fixed_hybrid_geocoder():\n",
        "    geocoder = FixedHybridGeocoder()\n",
        "    test_addresses = [\n",
        "        \"Salmiya, Block 1, Street 1\",\n",
        "        \"Mubarak Al-Kabeer, Block 2, St 34\",\n",
        "        \"Hawalli, Block 4, tunis street\",\n",
        "        \"Mishref, Block 4, Street 2\",\n",
        "        \"Salmiya, Block 12, Street 2\",\n",
        "        \"Hawalli, Beirut Street, Commercial Bank of Kuwait\",\n",
        "        \"Hawalli , Block 4 , tunis street, Hawalli Park\",\n",
        "        \"Salwa Block 11 street 10\",\n",
        "        \"Salmiya, Block 4, Street 2, building 14\",\n",
        "        \"Salmiya, Block 7, street 77,bldg 12\",\n",
        "        \"Salmiya, Block 10, Street 10, bldg 57\",\n",
        "        \"Salmiya, Block 10, bldg 57\",\n",
        "        \"Sabahiya, Block 4, 9 street,building 220\",\n",
        "        \"Zahra, Block 3, St 310,building 5\",\n",
        "        \"Rawda, Block 5, St 50, 12\",\n",
        "        \"Mubarak Al-Kabeer, Block 5, St 23, 9\"\n",
        "    ]\n",
        "    ground_truth = np.array([\n",
        "        (29.3492824, 48.0953218),\n",
        "        (29.1924, 48.0774),\n",
        "        (29.341525, 48.019292),\n",
        "        (29.2779, 48.0690),\n",
        "        (29.3246, 48.0568),\n",
        "        (29.335272277402016, 48.016834684876756),\n",
        "        (29.34037499802647, 48.02221585684463),\n",
        "        (29.284326847832105, 48.08411563806048),\n",
        "        (29.341837, 48.081363),\n",
        "        (29.33749, 48.0638),\n",
        "        (29.327933375139658, 48.06915761997239),\n",
        "        (29.327933375139658, 48.06915761997239),\n",
        "        (29.113487, 48.112334),\n",
        "        (29.2803, 47.9899),\n",
        "        (29.3302,47.994),\n",
        "        (29.189831,48.085122)\n",
        "    ])\n",
        "    results = geocoder.predict_coordinates_hybrid(test_addresses)\n",
        "    latitudes = np.array([r['latitude'] for r in results])\n",
        "    longitudes = np.array([r['longitude'] for r in results])\n",
        "    distances = haversine_distance(latitudes, longitudes, ground_truth[:, 0], ground_truth[:, 1])\n",
        "    for i, (result, distance) in enumerate(zip(results, distances), 1):\n",
        "        print(f\"{i:2d}. Input: {result['input']}\")\n",
        "        print(f\"    Coordinates: ({result['latitude']:.6f}, {result['longitude']:.6f})\")\n",
        "        print(f\"    Confidence: {result['confidence']}\")\n",
        "        print(f\"    Distance Error: {distance:.1f} m\")\n",
        "        if i < len(results):\n",
        "            print(\"\\n\" + \"=\"*60)\n",
        "    return results, []\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test_results, _ = test_fixed_hybrid_geocoder()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ruy0440O-mjG",
        "outputId": "e76e29ca-910b-45b9-db8d-7158998a0b72"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 1. Input: Salmiya, Block 1, Street 1\n",
            "    Coordinates: (29.349827, 48.096606)\n",
            "    Confidence: high\n",
            "    Distance Error: 138.4 m\n",
            "\n",
            "============================================================\n",
            " 2. Input: Mubarak Al-Kabeer, Block 2, St 34\n",
            "    Coordinates: (29.193239, 48.077762)\n",
            "    Confidence: high\n",
            "    Distance Error: 99.7 m\n",
            "\n",
            "============================================================\n",
            " 3. Input: Hawalli, Block 4, tunis street\n",
            "    Coordinates: (29.343455, 48.019452)\n",
            "    Confidence: high\n",
            "    Distance Error: 215.1 m\n",
            "\n",
            "============================================================\n",
            " 4. Input: Mishref, Block 4, Street 2\n",
            "    Coordinates: (29.277649, 48.069697)\n",
            "    Confidence: high\n",
            "    Distance Error: 73.1 m\n",
            "\n",
            "============================================================\n",
            " 5. Input: Salmiya, Block 12, Street 2\n",
            "    Coordinates: (29.327200, 48.055700)\n",
            "    Confidence: high\n",
            "    Distance Error: 308.1 m\n",
            "\n",
            "============================================================\n",
            " 6. Input: Hawalli, Beirut Street, Commercial Bank of Kuwait\n",
            "    Coordinates: (29.335661, 48.019771)\n",
            "    Confidence: medium\n",
            "    Distance Error: 287.9 m\n",
            "\n",
            "============================================================\n",
            " 7. Input: Hawalli , Block 4 , tunis street, Hawalli Park\n",
            "    Coordinates: (29.341747, 48.019785)\n",
            "    Confidence: high\n",
            "    Distance Error: 280.7 m\n",
            "\n",
            "============================================================\n",
            " 8. Input: Salwa Block 11 street 10\n",
            "    Coordinates: (29.283324, 48.083596)\n",
            "    Confidence: high\n",
            "    Distance Error: 122.3 m\n",
            "\n",
            "============================================================\n",
            " 9. Input: Salmiya, Block 4, Street 2, building 14\n",
            "    Coordinates: (29.341728, 48.081739)\n",
            "    Confidence: high\n",
            "    Distance Error: 38.4 m\n",
            "\n",
            "============================================================\n",
            "10. Input: Salmiya, Block 7, street 77,bldg 12\n",
            "    Coordinates: (29.337500, 48.064000)\n",
            "    Confidence: high\n",
            "    Distance Error: 19.4 m\n",
            "\n",
            "============================================================\n",
            "11. Input: Salmiya, Block 10, Street 10, bldg 57\n",
            "    Coordinates: (29.329074, 48.071066)\n",
            "    Confidence: high\n",
            "    Distance Error: 224.3 m\n",
            "\n",
            "============================================================\n",
            "12. Input: Salmiya, Block 10, bldg 57\n",
            "    Coordinates: (29.331431, 48.066160)\n",
            "    Confidence: medium\n",
            "    Distance Error: 485.5 m\n",
            "\n",
            "============================================================\n",
            "13. Input: Sabahiya, Block 4, 9 street,building 220\n",
            "    Coordinates: (29.111668, 48.110342)\n",
            "    Confidence: high\n",
            "    Distance Error: 280.0 m\n",
            "\n",
            "============================================================\n",
            "14. Input: Zahra, Block 3, St 310,building 5\n",
            "    Coordinates: (29.279279, 47.994298)\n",
            "    Confidence: high\n",
            "    Distance Error: 441.4 m\n",
            "\n",
            "============================================================\n",
            "15. Input: Rawda, Block 5, St 50, 12\n",
            "    Coordinates: (29.332058, 47.992915)\n",
            "    Confidence: high\n",
            "    Distance Error: 231.8 m\n",
            "\n",
            "============================================================\n",
            "16. Input: Mubarak Al-Kabeer, Block 5, St 23, 9\n",
            "    Coordinates: (29.189855, 48.084893)\n",
            "    Confidence: high\n",
            "    Distance Error: 22.4 m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls models/sentence_embedder"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLdnCm-svyWy",
        "outputId": "c4859f5e-f489-485e-9258-73c0aaeed9b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1_Pooling\t\t\t   sentence_bert_config.json\n",
            "config.json\t\t\t   special_tokens_map.json\n",
            "config_sentence_transformers.json  tokenizer_config.json\n",
            "model.safetensors\t\t   tokenizer.json\n",
            "modules.json\t\t\t   vocab.txt\n",
            "README.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls models\n"
      ],
      "metadata": {
        "id": "ZpHsSK7exDND",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7e9283b-c759-4fca-da73-2639191874c7"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "address_normalization_dicts.json  requirements.txt\n",
            "faiss_lat_index.index             residuals.npy\n",
            "faiss_lon_index.index             sample_weights.npy\n",
            "feature_dimensions.pkl            \u001b[0m\u001b[01;34msentence_embedder\u001b[0m/\n",
            "feature_scaler.pkl                tfidf_vectorizer.pkl\n",
            "feature_stats.json                training_metadata.json\n",
            "geo_stats.pkl                     training_results.json\n",
            "hybrid_xgbfaiss_geocoder.pkl      xgboost_geocoder.pkl\n",
            "manual_feature_columns.pkl        X_train_ref.npy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sAY73hbD9WlJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}